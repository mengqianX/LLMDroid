import csv
import glob
import json
import os
import subprocess
import time
from argparse import ArgumentParser, Namespace
from typing import List, Dict
from xml.dom import minidom

from xml.parsers.expat import ExpatError

ALL_APPS = ['ActivityDiary', 'AmazeFileManager', 'and-bible', 'AnkiDroid', 'APhotoManager', 'commons',
            'collect', 'FirefoxLite', 'Frost', 'geohashdroid', 'MaterialFBook', 'nextcloud', 'Omni-Notes',
            'open-event-attendee-android', 'openlauncher', 'osmeditor4android', 'Phonograph', 'Scarlet-Notes',
            'sunflower', 'WordPress',"AntennaPod","newpipe","duckduckgo",'redreader','myExpenses', 'bookdash', 'Wikipedia', 'kotatsu', 'aard2']
TOOL_NAMES = ['hybirddroid','fastbot','monkey','aurora','ape']


def get_app_name(testing_result_dir):
    for app_name in ALL_APPS:
        if os.path.basename(testing_result_dir).startswith(app_name):
            return app_name
    print("Warning: cannot find app name for this testing result dir: %s" % testing_result_dir)


def get_apk_name(testing_result_dir: str):
    base_name = os.path.basename(testing_result_dir)
    target_apk_file_name = str(base_name.split(".apk")[0]) + ".apk"
    # target_apk_file_name = str(base_name.split("_")[0]) + ".apk"
    return target_apk_file_name


def get_issue_id(testing_result_dir: str):
    base_name = os.path.basename(testing_result_dir)
    issue_id_str = base_name.split("#")[1].split(".")[0]
    return str(issue_id_str)


def read_coverage_jacoco(jacoco_report_file):
    if not os.path.isfile(jacoco_report_file):
        return False, 0, 0, 0, 0

    try:
        # see the format of coverage report generated by Jacoco in xml
        xmldoc = minidom.parse(jacoco_report_file)
        counters = xmldoc.getElementsByTagName('counter')

        line_coverage = 0
        branch_coverage = 0
        method_coverage = 0
        class_coverage = 0

        for counter in counters:
            type_name = counter.getAttribute('type')
            missed_items = int(counter.getAttribute('missed'))
            covered_items = int(counter.getAttribute('covered'))

            if type_name == 'LINE':
                line_coverage = covered_items * 100.0 / (missed_items + covered_items)

            if type_name == 'BRANCH':
                branch_coverage = covered_items * 100.0 / (missed_items + covered_items)

            if type_name == 'METHOD':
                method_coverage = covered_items * 100.0 / (missed_items + covered_items)

            if type_name == 'CLASS':
                class_coverage = covered_items * 100.0 / (missed_items + covered_items)

        print("-----------")
        print("Line: " + str(line_coverage) + ", Branch: " + str(branch_coverage) + ", Method: " + str(method_coverage)
              + ", Class: " + str(class_coverage))
        print("-----------")
        return True, float("{:.2f}".format(line_coverage)), float("{:.2f}".format(branch_coverage)), \
               float("{:.2f}".format(method_coverage)), float("{:.2f}".format(class_coverage))
    except ExpatError:
        print("*****Parse xml error, catch it!********")
        return False, 0, 0, 0, 0


def get_class_source_files_dirs(app_name, target_apk_file_name):
    class_files = os.path.join("../" + app_name, "class_files.json")
    assert os.path.exists(class_files)

    tmp_file = open(class_files, "r")
    tmp_file_dict = json.load(tmp_file)
    tmp_file.close()

    # Get the class and source files #
    class_source_files_dict = tmp_file_dict[target_apk_file_name]

    class_files_dirs = class_source_files_dict['classfiles']
    source_files_dirs = class_source_files_dict['sourcefiles']

    assert len(class_files_dirs) != 0 and len(source_files_dirs) != 0

    return class_files_dirs, source_files_dirs


def get_class_files_str(app_name, class_files_dirs):
    class_files_dirs_str = ""
    for tmp_dir in class_files_dirs:
        # 使用 os.path.join 并确保路径中的反斜杠正确处理
        class_files_dirs_str += " --classfiles " + os.path.normpath(os.path.join("..", app_name, tmp_dir))
        # class_files_dirs_str += " --classfiles " + os.path.join("../" + app_name, tmp_dir)
    return class_files_dirs_str

def get_valid_ec_file(coverage_data_dir):
    # Get the coverage data files #
    coverage_ec_files = [os.path.join(coverage_data_dir, f) for f in os.listdir(coverage_data_dir) if
                         os.path.isfile(os.path.join(coverage_data_dir, f)) and f.startswith('valid')] 
    return coverage_ec_files  


def merge_before_coverage_ec_files(coverage_data_dir):
    # Get the coverage data files #
    valid_coverage_ec_files = [os.path.join(coverage_data_dir, f) for f in os.listdir(coverage_data_dir) if
                         os.path.isfile(os.path.join(coverage_data_dir, f)) and f.endswith('.ec') and f.startswith('valid')]
    if valid_coverage_ec_files:
        return 
    
    coverage_ec_files = [os.path.join(coverage_data_dir, f) for f in os.listdir(coverage_data_dir) if
                         os.path.isfile(os.path.join(coverage_data_dir, f)) and f.endswith('.ec') and not f.startswith('valid')]
    coverage_ec_files_str = ""
    count = 1
    for ec_file in coverage_ec_files:
        coverage_ec_files_str += " " + ec_file
        merged_coverage_ec_file_path = os.path.join(coverage_data_dir, "valid_coverage_all_{}.ec".format(count))
        count += 1

        if not os.path.exists(merged_coverage_ec_file_path):
            # only merge when the "coverage_all.ec" does not exist

            merge_cmd = "java -jar ../tools/jacococli.jar merge " + coverage_ec_files_str + \
                    " --destfile " + merged_coverage_ec_file_path
            print('$ %s' % merge_cmd)

            try:
                p = subprocess.Popen(merge_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
                # clear the output
                output = p.communicate()[0].decode('latin1').strip()   
                # output = p.communicate()[0].decode('utf-8').strip()
                print(output)
            except os.error as e:
                print(e)
    return



def compute_code_coverage(app_name, tool_name, testing_result_dir, coverage_data_dir):
    target_apk_file_name = get_apk_name(testing_result_dir)

    class_files_dirs, source_files_dirs = get_class_source_files_dirs(app_name, target_apk_file_name)

    class_files_dirs_str = get_class_files_str(app_name, class_files_dirs)

    coverage_ec_files_str = coverage_data_dir

    # handle the case when no coverage data is available
    if len(coverage_ec_files_str) == 0:
        return False, 0, 0, 0, 0

    # Assemble and execute the coverage computation command #
    xml_coverage_report_file_path = os.path.join(testing_result_dir, "coverage_report.xml")
    cmd = "java -jar ../tools/jacococli.jar report " + coverage_ec_files_str + class_files_dirs_str + " --xml " + \
          xml_coverage_report_file_path
    print('$ %s' % cmd)

    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    # clear the output
    output = p.communicate()[0].decode('latin1').strip()    
    # output = p.communicate()[0].decode('utf-8').strip()
    print(output)

    return read_coverage_jacoco(xml_coverage_report_file_path)

def compute_average_code_coverage(target_app_testing_result_dirs):
    
    for coverage_dir in target_app_testing_result_dirs:
        coverage_csv = os.path.join(coverage_dir,"all_coverage.csv")
        df = pd.read_csv(coverage_csv, header=None)
        # 对第三列的文件路径进行排序
        df['File Number'] = df.iloc[:, 2].apply(extract_file_number)
        df_sorted = df.sort_values(by='File Number')
        coverage_data = df_sorted.iloc[:, 3].reset_index(drop=True)
        # 只提取一列数据出来，会变成series对象，需要再转换为dataframe对象
        coverage_data = pd.DataFrame(coverage_data)

def get_tool_name_from_csv(csv_dir):
    base_name = os.path.basename(csv_dir)
    base_name = str(base_name.split(".")[0])
    tool_name = str(base_name.split("_")[-1])
    return tool_name

def get_tool_name(testing_result_dir: str):
    base_name = os.path.basename(testing_result_dir)
    base_name = str(base_name.split(".apk.")[1])
    tool_name = str(base_name.split(".result")[0])
    return tool_name

import os
import subprocess
import pandas as pd
import matplotlib.pyplot as plt
from xml.dom import minidom

def get_average_from_different_coverage_category(df):

    # 每隔4列为1组，计算每组的平均值，分别表示line，branch，method，class
    average_columns = []
    for i in range(4):
        # 选择每组的列
        group_columns = df.iloc[:, i::4]  # 选择第i列及之后每隔4列的列
        average_columns.append(group_columns.mean(axis=1))  # 计算每组的平均值

    # 将平均值添加到 DataFrame
    for index, avg in enumerate(average_columns):
        df[f'Average_Column_{index + 1}'] = avg
    return df

def get_all_average_coverage_csv_for_each_tool(root_dir,tool_name):
        # 检查目录是否存在
    if not os.path.exists(root_dir):
        print(f"目录不存在: {root_dir}")
    else:
        print(f"目录存在: {root_dir}")
    # 指定要读取的目录
    # root_dir = 'path/to/your/directory'
    output_file = os.path.join(root_dir,'all_average_coverage_{}.csv'.format(tool_name))

    # 用于存放所有数据的列表
    all_columns = []

    # 遍历目录
    for subdir, _, files in os.walk(root_dir):    
        print("当前目录:", subdir)  # 打印当前遍历的目录    
        if 'all_coverage.csv' in files and get_tool_name(subdir) == tool_name :
            file_path = os.path.join(subdir, 'all_coverage.csv')
            print("找到文件:", file_path)  # 打印找到的文件
            df = pd.read_csv(file_path, header=None)
            # 对第三列的文件路径进行排序
            df['File Number'] = df.iloc[:, 2].apply(extract_file_number)
            df_sorted = df.sort_values(by='File Number')
            coverage_data = df_sorted.iloc[:,3:7].reset_index(drop=True)  # 选择第4列往后4列，表示所有covergae数据
            coverage_data = pd.DataFrame(coverage_data)
            all_columns.append(coverage_data)
    if len(all_columns) < 1:
        return
    print("收集的列：", all_columns)
    # 横向合并所有的第一列
    combined_df = pd.concat(all_columns, axis=1)

    # 用最后一个有效值填充缺失值
    combined_df.fillna(method='ffill', inplace=True)
    # 计算不同类别coverage的平均值，放在最后4列
    combined_df = get_average_from_different_coverage_category(combined_df)

    # 保存结果到新的CSV文件
    combined_df.to_csv(output_file, index=False,  header=False)

    print(f'所有数据和平均值已保存到 {output_file}')
    return combined_df

def get_average_line_coverage_csv(root_dir,tool_name):
    # 检查目录是否存在
    if not os.path.exists(root_dir):
        print(f"目录不存在: {root_dir}")
    else:
        print(f"目录存在: {root_dir}")
    # 指定要读取的目录
    # root_dir = 'path/to/your/directory'
    output_file = os.path.join(root_dir,'average_line_coverage_{}.csv'.format(tool_name))

    # 用于存放所有数据的列表
    all_columns = []

    # 遍历目录
    for subdir, _, files in os.walk(root_dir):    
        print("当前目录:", subdir)  # 打印当前遍历的目录    
        if 'all_coverage.csv' in files and get_tool_name(subdir) == tool_name :
            file_path = os.path.join(subdir, 'all_coverage.csv')
            print("找到文件:", file_path)  # 打印找到的文件
            df = pd.read_csv(file_path, header=None)
            # 对第三列的文件路径进行排序
            df['File Number'] = df.iloc[:, 2].apply(extract_file_number)
            df_sorted = df.sort_values(by='File Number')
            coverage_data = df_sorted.iloc[:, 3].reset_index(drop=True)  # 选择第4列
            coverage_data = pd.DataFrame(coverage_data)
            all_columns.append(coverage_data)
    if len(all_columns) < 1:
        return
    print("收集的列：", all_columns)
    # 横向合并所有的第一列
    combined_df = pd.concat(all_columns, axis=1)

    # 用最后一个有效值填充缺失值
    combined_df.fillna(method='ffill', inplace=True)

    # 计算每一行的平均值
    averaged_df = combined_df.mean(axis=1)

    # 将平均值添加到合并后的 DataFrame 中
    combined_df['Average'] = averaged_df

    # 保存结果到新的CSV文件
    combined_df.to_csv(output_file, index=False,  header=False)

    print(f'所有line_coverage数据和平均值已保存到 {output_file}')
    return combined_df

def get_average_branch_coverage_csv(root_dir,tool_name):
    # 检查目录是否存在
    if not os.path.exists(root_dir):
        print(f"目录不存在: {root_dir}")
    else:
        print(f"目录存在: {root_dir}")
    # 指定要读取的目录
    # root_dir = 'path/to/your/directory'
    output_file = os.path.join(root_dir,'average_branch_coverage_{}.csv'.format(tool_name))

    # 用于存放所有数据的列表
    all_columns = []

    # 遍历目录
    for subdir, _, files in os.walk(root_dir):    
        print("当前目录:", subdir)  # 打印当前遍历的目录    
        if 'all_coverage.csv' in files and get_tool_name(subdir) == tool_name :
            file_path = os.path.join(subdir, 'all_coverage.csv')
            print("找到文件:", file_path)  # 打印找到的文件
            df = pd.read_csv(file_path, header=None)
            # 对第三列的文件路径进行排序
            df['File Number'] = df.iloc[:, 2].apply(extract_file_number)
            df_sorted = df.sort_values(by='File Number')
            coverage_data = df_sorted.iloc[:, 4].reset_index(drop=True)  # 选择第5列 branch
            coverage_data = pd.DataFrame(coverage_data)
            all_columns.append(coverage_data)
    if len(all_columns) < 1:
        return
    print("收集的列：", all_columns)
    # 横向合并所有的第一列
    combined_df = pd.concat(all_columns, axis=1)

    # 用最后一个有效值填充缺失值
    combined_df.fillna(method='ffill', inplace=True)

    # 计算每一行的平均值
    averaged_df = combined_df.mean(axis=1)

    # 将平均值添加到合并后的 DataFrame 中
    combined_df['Average'] = averaged_df

    # 保存结果到新的CSV文件
    combined_df.to_csv(output_file, index=False,  header=False)

    print(f'所有branch_coverage数据和平均值已保存到 {output_file}')
    return combined_df

def get_combined_all_and_average_coverage_csv(root_dir):
    all_average_coverage_csvs = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if
                         os.path.isfile(os.path.join(root_dir, f)) and f.endswith('.csv') and f.startswith('all_average')]
       # 指定要读取的目录
    # root_dir = 'path/to/your/directory'
    output_file = os.path.join(root_dir,'combined_all_and_average_coverage.csv')

    # 用于存放所有数据的列表
    all_columns = []
    columns = []
    # 遍历目录
    for all_average_coverage_csv in all_average_coverage_csvs:
        print("find csv: ", all_average_coverage_csv)
        tool_name = get_tool_name_from_csv(all_average_coverage_csv)
        df = pd.read_csv(all_average_coverage_csv, header=None)
        all_columns.append(df)
        # 添加5个 tool_name 到 columns 列表(因为有5轮实验)
        for i in range(20):
            if i%4 == 0:
                columns.append('{}_line_cov'.format(tool_name))
            elif i%4 == 1:
                columns.append('{}_branch_cov'.format(tool_name))
            elif i%4 == 2:
                columns.append('{}_method_cov'.format(tool_name))
            elif i%4 == 3:
                columns.append('{}_class_cov'.format(tool_name))

        columns.append('average_{}_line_cov'.format(tool_name))
        columns.append('average_{}_branch_cov'.format(tool_name))
        columns.append('average_{}_method_cov'.format(tool_name))
        columns.append('average_{}_class_cov'.format(tool_name))

    # 横向合并所有的第一列
    combined_df = pd.concat(all_columns, axis=1)
    # 用最后一个有效值填充缺失值
    combined_df.fillna(method='ffill', inplace=True)
    combined_df.columns = columns
    # 保存结果到新的CSV文件
    combined_df.to_csv(output_file, index=False)

    print(f'所有数据和平均值已保存到 {output_file}')
    return combined_df

def get_combined_only_average_coverage_csv(root_dir):
    all_average_coverage_csvs = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if
                         os.path.isfile(os.path.join(root_dir, f)) and f.endswith('.csv') and f.startswith('all_average')]
       # 指定要读取的目录
    # root_dir = 'path/to/your/directory'
    output_file = os.path.join(root_dir,'combined_only_average_coverage.csv')

    # 用于存放所有数据的列表
    all_columns = []
    columns = []
    # 遍历目录
    for all_average_coverage_csv in all_average_coverage_csvs:
        print("find csv: ", all_average_coverage_csv)
        tool_name = get_tool_name_from_csv(all_average_coverage_csv)
        df = pd.read_csv(all_average_coverage_csv, header=None)
        coverage_data = df.iloc[:, -4:].reset_index(drop=True)  # 选择第4列
        # coverage_data = pd.DataFrame(coverage_data)
        all_columns.append(coverage_data)
        columns.append('average_{}_line_cov'.format(tool_name))
        columns.append('average_{}_branch_cov'.format(tool_name))
        columns.append('average_{}_method_cov'.format(tool_name))
        columns.append('average_{}_class_cov'.format(tool_name))

    # 横向合并所有的第一列
    combined_df = pd.concat(all_columns, axis=1)
    # 用最后一个有效值填充缺失值
    combined_df.fillna(method='ffill', inplace=True)
    combined_df.columns = columns
    # 保存结果到新的CSV文件
    combined_df.to_csv(output_file, index=False)

    print(f'所有数据和平均值已保存到 {output_file}')
    return combined_df

def get_timestamp(testing_result_dir: str):
    base_name = os.path.basename(testing_result_dir)
    timestamp = str(base_name.split("#")[-1])
    return timestamp

def get_app_name_from_output(output_dir):
    app_name_dir = os.path.dirname(output_dir)
    app_name = os.path.basename(app_name_dir)
    return app_name

def plot_line_coverage(root_dir):
    """绘制覆盖率随时间变化的折线图"""
    print("开始绘制line coverage")
    title = ""
    # 绘制折线图
    plt.figure(figsize=(12, 6))

    average_coverage_csv = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if
                         os.path.isfile(os.path.join(root_dir, f)) and f.endswith('.csv') and f.startswith('average_line_')]

    # all_coverage_csv = []
    time_intervals = []

    # 对每一个tool绘制
    for coverage_csv in average_coverage_csv:
        tool_name = get_tool_name_from_csv(coverage_csv)
        print("find average csv: ",coverage_csv)
        df = pd.read_csv(coverage_csv, header=None)
        coverage_data = df.iloc[:, -1].reset_index(drop=True)  # 选择最后一列
        # 确保长度为 36
        if len(coverage_data) < 36:
            # 填充到 36
            last_value = coverage_data.iloc[-1]  # 获取最后一个值
            coverage_data = coverage_data.tolist() + [last_value] * (36 - len(coverage_data))
        else:
            # 截断到 36
            coverage_data = coverage_data.iloc[:36].tolist()
        # 只提取一列数据出来，会变成series对象，需要再转换为dataframe对象
        coverage_data = pd.DataFrame(coverage_data)
        # 设置列名
        column = tool_name
        coverage_data.columns = [column]
        # print(coverage_data.columns)
        # print(coverage_data)

        # 检查数据是否有效
        if coverage_data.empty:
            print(f"No data available in {coverage_csv}. Skipping.")
            continue
        
        # all_coverage_csv.append(coverage_data)
        
        # 创建时间序列，从0开始，每隔5分钟
        time_intervals = [i * 5 for i in range(len(coverage_data))]
        # 绘制覆盖率
        # plt.plot(time_intervals, coverage_data[column], marker='o', label=tool_name, 
        #          color='blue' if tool_name == 'hybirddroid' else 
        #                'orange' if tool_name == 'fastbot' else 
        #                'green' if tool_name == 'monkey' else 
        #                'gray' if tool_name == 'aurora' else
        #                'red' if tool_name == 'ape' else
        #                'white')
        plt.plot(time_intervals, coverage_data[column], marker='o', label=tool_name, 
         color='#1f77b4' if tool_name == 'hybirddroid' else 
               '#ff7f0e' if tool_name == 'fastbot' else 
               '#2ca02c' if tool_name == 'monkey' else 
               '#d62728' if tool_name == 'aurora' else 
               '#9467bd' if tool_name == 'ape' else 
               '#7f7f7f')
    app_name = get_app_name_from_output(root_dir)
    # # 横向合并所有的第一列
    # combined_df = pd.concat(all_coverage_csv, axis=1)
    # # 用最后一个有效值填充缺失值
    # combined_df.fillna(method='ffill', inplace=True)
    # # 保存结果到新的CSV文件
    # combined_df.to_csv(output_file, index=False, header=True)

    title = 'Average Line Coverage for {} with {} run times'.format(app_name, runcounts)
    plt.title(title)
    plt.xlabel('Time (minutes)')
    plt.ylabel('Coverage Percentage (%)')
    # plt.xticks(coverage_data.index + 1)  # 设置 x 轴为覆盖计数
    plt.xticks(time_intervals)  # 设置 x 轴为时间间隔
    plt.legend()
    plt.grid()
    plt.tight_layout()
    # 保存折线图
    plt.savefig(r'D:\\GitHubRepo\\themis\wanlaoshi\\output\\coverage_plot\\{}.png'.format(title), dpi=300)  # 保存为 PNG 格式，分辨率为 300 DPI
    print("line'coverage绘制完成！")
    # plt.show()  

def plot_branch_coverage(root_dir):
    """绘制覆盖率随时间变化的折线图"""
    title = ""
    # 绘制折线图
    plt.figure(figsize=(12, 6))

    average_coverage_csv = [os.path.join(root_dir, f) for f in os.listdir(root_dir) if
                         os.path.isfile(os.path.join(root_dir, f)) and f.endswith('.csv') and f.startswith('average_branch_')]

    # all_coverage_csv = []
    time_intervals = []

    # 对每一个tool绘制
    for coverage_csv in average_coverage_csv:
        tool_name = get_tool_name_from_csv(coverage_csv)
        print("tool name label:",tool_name)
        print("find average csv: ",coverage_csv)
        df = pd.read_csv(coverage_csv, header=None)
        coverage_data = df.iloc[:, -1].reset_index(drop=True)  # 选择最后一列
        # 确保长度为 36
        if len(coverage_data) < 36:
            # 填充到 36
            last_value = coverage_data.iloc[-1]  # 获取最后一个值
            coverage_data = coverage_data.tolist() + [last_value] * (36 - len(coverage_data))
        else:
            # 截断到 36
            coverage_data = coverage_data.iloc[:36].tolist()
        # 只提取一列数据出来，会变成series对象，需要再转换为dataframe对象
        coverage_data = pd.DataFrame(coverage_data,columns=[tool_name])
        # 设置列名
        # column = tool_name
        # coverage_data.columns = [column]
        # print(coverage_data.columns)
        # print(coverage_data)

        # 检查数据是否有效
        if coverage_data.empty:
            print(f"No data available in {coverage_csv}. Skipping.")
            continue
        
        # all_coverage_csv.append(coverage_data)
        
        # 创建时间序列，从0开始，每隔5分钟
        time_intervals = [i * 5 for i in range(len(coverage_data))]
        # 绘制覆盖率
        # plt.plot(time_intervals, coverage_data[tool_name], marker='o', label=tool_name, 
        #          color='blue' if tool_name == 'hybirddroid' else 
        #                'orange' if tool_name == 'fastbot' else 
        #                'green' if tool_name == 'monkey' else 
        #                'gray')
        plt.plot(time_intervals, coverage_data[tool_name], marker='o', label=tool_name, 
         color='#1f77b4' if tool_name == 'hybirddroid' else 
               '#ff7f0e' if tool_name == 'fastbot' else 
               '#2ca02c' if tool_name == 'monkey' else 
               '#d62728' if tool_name == 'aurora' else 
               '#9467bd' if tool_name == 'ape' else 
               '#7f7f7f')
    app_name = get_app_name_from_output(root_dir)
    # # 横向合并所有的第一列
    # combined_df = pd.concat(all_coverage_csv, axis=1)
    # # 用最后一个有效值填充缺失值
    # combined_df.fillna(method='ffill', inplace=True)
    # # 保存结果到新的CSV文件
    # combined_df.to_csv(output_file, index=False, header=True)

    title = 'Average Branch Coverage for {} with {} run times'.format(app_name, runcounts)
    plt.title(title)
    plt.xlabel('Time (minutes)')
    plt.ylabel('Coverage Percentage (%)')
    # plt.xticks(coverage_data.index + 1)  # 设置 x 轴为覆盖计数
    plt.xticks(time_intervals)  # 设置 x 轴为时间间隔
    plt.legend()
    plt.grid()
    plt.tight_layout()
    # 保存折线图
    plt.savefig(r'D:\\GitHubRepo\\themis\wanlaoshi\\output\\coverage_plot\\{}.png'.format(title), dpi=300)  # 保存为 PNG 格式，分辨率为 300 DPI

    # plt.show()  

import pandas as pd
import matplotlib.pyplot as plt
import re

# 读取 CSV 文件
# 提取文件名并排序
def extract_file_number(file_path):
    # 使用正则表达式提取文件名中的数字
    match = re.search(r'valid_coverage_all_(\d+)\.ec$', os.path.basename(file_path))
    return int(match.group(1)) if match else float('inf')

runcounts=5

if __name__ == "__main__":
    ap = ArgumentParser()
    ap.add_argument('-dir', type=str, dest='coverage_dir', help='the testing root dir')
    arg = ap.parse_args()
    coverage_dir = arg.coverage_dir
    for tool_name in TOOL_NAMES:
        get_average_line_coverage_csv(coverage_dir,tool_name)
        get_average_branch_coverage_csv(coverage_dir,tool_name)
        get_all_average_coverage_csv_for_each_tool(coverage_dir,tool_name)
    get_combined_all_and_average_coverage_csv(coverage_dir)
    get_combined_only_average_coverage_csv(coverage_dir)
    plot_line_coverage(coverage_dir)
    plot_branch_coverage(coverage_dir)


